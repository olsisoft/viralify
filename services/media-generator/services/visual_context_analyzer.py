"""
Visual Context Analyzer - GPT-4 powered scene analysis for optimal visual generation.

Analyzes scene descriptions to determine:
- Best visual type (diagram, chart, avatar, concept, stock)
- Specific diagram type if applicable (flowchart, sequence, architecture, etc.)
- Whether Mermaid.js can accurately represent the content
- Key elements and relationships for diagram generation
"""

import json
import logging
from typing import Optional, List, Dict, Any
from openai import AsyncOpenAI

# Use shared LLM provider for model name resolution
try:
    from shared.llm_provider import get_model_name as _get_model_name
    _HAS_SHARED_LLM = True
except ImportError:
    _HAS_SHARED_LLM = False
    _get_model_name = lambda tier: {"fast": "gpt-4o-mini", "quality": "gpt-4o"}.get(tier, "gpt-4o-mini")

from models.visual_types import (
    VisualType,
    DiagramType,
    ChartType,
    VisualAnalysis,
)

logger = logging.getLogger(__name__)


class VisualContextAnalyzer:
    """Analyzes scene context to determine optimal visual generation strategy."""

    # Domains that typically require technical diagrams
    TECHNICAL_DOMAINS = [
        "software", "architecture", "engineering", "system",
        "database", "api", "network", "cloud", "devops",
        "microservices", "infrastructure", "data flow"
    ]

    # Keywords suggesting diagram content
    DIAGRAM_KEYWORDS = [
        "process", "flow", "steps", "workflow", "pipeline",
        "architecture", "structure", "components", "layers",
        "sequence", "interaction", "communication", "protocol",
        "relationship", "hierarchy", "tree", "graph",
        "state", "transition", "lifecycle", "phases"
    ]

    # Keywords suggesting avatar/presenter content
    AVATAR_KEYWORDS = [
        "explain", "present", "introduce", "welcome",
        "host", "narrator", "guide", "instructor",
        "talk", "speak", "discuss", "describe"
    ]

    def __init__(self, openai_api_key: str):
        self.client = AsyncOpenAI(api_key=openai_api_key)

    async def analyze_scene(
        self,
        description: str,
        script_context: Optional[str] = None,
        full_script: Optional[str] = None
    ) -> VisualAnalysis:
        """
        Analyze a scene description to determine the optimal visual type.

        Args:
            description: The scene description to analyze
            script_context: Surrounding script context for better understanding
            full_script: Full video script for global context

        Returns:
            VisualAnalysis with recommendations
        """
        try:
            # Build context for GPT-4
            context_parts = []
            if script_context:
                context_parts.append(f"Script context: {script_context}")
            if full_script:
                context_parts.append(f"Full script summary: {full_script[:500]}...")

            context = "\n".join(context_parts) if context_parts else "No additional context"

            # Call GPT-4 for analysis
            response = await self.client.chat.completions.create(
                model=_get_model_name("quality"),
                messages=[
                    {
                        "role": "system",
                        "content": self._get_system_prompt()
                    },
                    {
                        "role": "user",
                        "content": self._get_analysis_prompt(description, context)
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=1500
            )

            result = json.loads(response.choices[0].message.content)
            return self._parse_analysis_result(result, description)

        except Exception as e:
            logger.error(f"Visual analysis failed: {e}")
            # Return fallback analysis
            return self._fallback_analysis(description)

    def _get_system_prompt(self) -> str:
        """System prompt for GPT-4 visual analysis."""
        return """You are a visual content strategist for video production. Your job is to analyze scene descriptions and determine the optimal type of visual content.

You must decide between these visual types:
1. DIAGRAM - Technical diagrams (flowcharts, architecture, sequences, etc.) that can be rendered with Mermaid.js
2. CHART - Data visualizations (bar charts, line graphs, pie charts)
3. AVATAR - Talking head presenter for explanations, introductions, or personal content
4. CONCEPT - Abstract or creative visuals best generated by DALL-E
5. STOCK - Real-world footage from stock video libraries
6. AI_IMAGE - General AI-generated images for specific scenes

For DIAGRAM type, also specify which diagram subtype:
- flowchart: Process flows, decision trees, workflows
- sequence: API calls, message flows, interactions between entities
- architecture: System components, microservices, infrastructure
- class: OOP classes, relationships, inheritance
- er: Database schemas, entity relationships
- mindmap: Hierarchical concepts, brainstorming
- state: State machines, transitions
- gantt: Timelines, project schedules

IMPORTANT RULES:
1. Choose DIAGRAM only if the content is truly technical and can be accurately represented by Mermaid.js
2. Choose AVATAR for scenes that describe a presenter, host, or when personal delivery adds value
3. Choose CONCEPT for abstract ideas, metaphors, or artistic representations
4. Choose STOCK for real-world scenes, environments, or actions
5. Extract specific elements and relationships when DIAGRAM is chosen
6. Provide enhanced DALL-E prompts when CONCEPT or AI_IMAGE is chosen

Respond in JSON format only."""

    def _get_analysis_prompt(self, description: str, context: str) -> str:
        """User prompt for scene analysis."""
        return f"""Analyze this video scene and determine the optimal visual type.

SCENE DESCRIPTION:
"{description}"

CONTEXT:
{context}

Respond with JSON containing:
{{
    "visual_type": "DIAGRAM|CHART|AVATAR|CONCEPT|STOCK|AI_IMAGE",
    "confidence": 0.0-1.0,
    "diagram_type": "flowchart|sequence|architecture|class|er|mindmap|state|gantt|null",
    "chart_type": "bar|line|pie|scatter|area|heatmap|null",
    "requires_avatar": true/false,
    "mermaid_possible": true/false,
    "domain": "software|architecture|business|data|education|marketing|null",
    "extracted_elements": ["element1", "element2", ...],
    "extracted_relationships": [
        {{"from": "A", "to": "B", "label": "connects to"}},
        ...
    ],
    "reasoning": "Brief explanation of why this visual type was chosen",
    "suggested_prompt": "Enhanced DALL-E prompt if using AI generation"
}}"""

    def _parse_analysis_result(self, result: Dict[str, Any], description: str) -> VisualAnalysis:
        """Parse GPT-4 response into VisualAnalysis model."""
        try:
            # Map string to enum
            visual_type_str = result.get("visual_type", "STOCK").upper()
            visual_type = VisualType(visual_type_str.lower())

            # Parse diagram type if applicable
            diagram_type = None
            if visual_type == VisualType.DIAGRAM and result.get("diagram_type"):
                try:
                    diagram_type = DiagramType(result["diagram_type"].lower())
                except ValueError:
                    diagram_type = DiagramType.FLOWCHART  # Default

            # Parse chart type if applicable
            chart_type = None
            if visual_type == VisualType.CHART and result.get("chart_type"):
                try:
                    chart_type = ChartType(result["chart_type"].lower())
                except ValueError:
                    chart_type = ChartType.BAR  # Default

            return VisualAnalysis(
                visual_type=visual_type,
                confidence=float(result.get("confidence", 0.7)),
                diagram_type=diagram_type,
                chart_type=chart_type,
                requires_avatar=bool(result.get("requires_avatar", False)),
                extracted_elements=result.get("extracted_elements", []),
                extracted_relationships=result.get("extracted_relationships", []),
                mermaid_possible=bool(result.get("mermaid_possible", False)),
                domain=result.get("domain"),
                reasoning=result.get("reasoning", ""),
                suggested_prompt=result.get("suggested_prompt")
            )

        except Exception as e:
            logger.error(f"Failed to parse analysis result: {e}")
            return self._fallback_analysis(description)

    def _fallback_analysis(self, description: str) -> VisualAnalysis:
        """
        Fallback analysis using keyword matching when GPT-4 fails.
        """
        description_lower = description.lower()

        # Check for diagram keywords
        is_technical = any(kw in description_lower for kw in self.TECHNICAL_DOMAINS)
        has_diagram_keywords = any(kw in description_lower for kw in self.DIAGRAM_KEYWORDS)

        # Check for avatar keywords
        has_avatar_keywords = any(kw in description_lower for kw in self.AVATAR_KEYWORDS)

        # Determine visual type
        if is_technical and has_diagram_keywords:
            visual_type = VisualType.DIAGRAM
            diagram_type = self._guess_diagram_type(description_lower)
            mermaid_possible = True
        elif has_avatar_keywords:
            visual_type = VisualType.AVATAR
            diagram_type = None
            mermaid_possible = False
        else:
            # Default to stock for most content
            visual_type = VisualType.STOCK
            diagram_type = None
            mermaid_possible = False

        return VisualAnalysis(
            visual_type=visual_type,
            confidence=0.5,  # Lower confidence for fallback
            diagram_type=diagram_type,
            requires_avatar=has_avatar_keywords,
            mermaid_possible=mermaid_possible,
            reasoning="Fallback analysis based on keyword matching"
        )

    def _guess_diagram_type(self, description: str) -> DiagramType:
        """Guess diagram type based on keywords."""
        if any(kw in description for kw in ["sequence", "api", "call", "message", "request"]):
            return DiagramType.SEQUENCE
        elif any(kw in description for kw in ["architecture", "system", "component", "microservice"]):
            return DiagramType.ARCHITECTURE
        elif any(kw in description for kw in ["class", "object", "inherit", "interface"]):
            return DiagramType.CLASS
        elif any(kw in description for kw in ["database", "entity", "table", "relation"]):
            return DiagramType.ER
        elif any(kw in description for kw in ["state", "transition", "lifecycle"]):
            return DiagramType.STATE
        elif any(kw in description for kw in ["timeline", "schedule", "gantt", "project"]):
            return DiagramType.GANTT
        elif any(kw in description for kw in ["mind", "concept", "idea", "brainstorm"]):
            return DiagramType.MINDMAP
        else:
            return DiagramType.FLOWCHART  # Default

    async def batch_analyze(
        self,
        scenes: List[Dict[str, str]],
        full_script: Optional[str] = None
    ) -> List[VisualAnalysis]:
        """
        Analyze multiple scenes efficiently.

        Args:
            scenes: List of {"description": str, "context": str}
            full_script: Full script for global context

        Returns:
            List of VisualAnalysis for each scene
        """
        results = []
        for scene in scenes:
            analysis = await self.analyze_scene(
                description=scene.get("description", ""),
                script_context=scene.get("context"),
                full_script=full_script
            )
            results.append(analysis)
        return results

    def should_use_mermaid(self, analysis: VisualAnalysis) -> bool:
        """Determine if Mermaid.js should be used for this visual."""
        return (
            analysis.visual_type == VisualType.DIAGRAM and
            analysis.mermaid_possible and
            analysis.confidence >= 0.6
        )

    def should_use_avatar(self, analysis: VisualAnalysis) -> bool:
        """Determine if avatar presenter should be used."""
        return (
            analysis.visual_type == VisualType.AVATAR or
            analysis.requires_avatar
        )
