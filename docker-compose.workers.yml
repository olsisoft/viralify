# ===========================================
# VIRALIFY WORKERS - Standalone Deployment
# ===========================================
# Deploy this on separate worker servers
# Connects to main server's RabbitMQ and Redis
#
# Usage:
#   1. Copy .env.workers.example to .env.workers and configure
#   2. Run with --env-file flag (Docker Compose uses .env by default, not .env.workers):
#
#      docker compose -f docker-compose.workers.yml --env-file .env.workers up -d
#
#   Or create a symlink:
#      ln -sf .env.workers .env
#      docker compose -f docker-compose.workers.yml up -d
#
# Required environment variables (in .env.workers):
#   - MAIN_SERVER_HOST: IP/hostname of main server (e.g., 10.0.0.1)
#   - WORKER_ID: Unique identifier for this worker server (e.g., "server1", "server2")
#                Required for Redis Streams consumer uniqueness across servers
#   - RABBITMQ_USER, RABBITMQ_PASSWORD
#   - REDIS_PASSWORD
#   - OPENAI_API_KEY (or other LLM provider keys)
#   - MINIO_ROOT_USER, MINIO_ROOT_PASSWORD (for object storage)
#
# Scaling (in .env.workers):
#   - COURSE_REPLICAS=6         # Number of course job consumers - RabbitMQ (default: 1)
#   - LECTURE_REPLICAS=4        # Number of lecture job consumers - Redis Streams (default: 1)
#   - FINALIZATION_REPLICAS=1   # Number of finalization workers - Redis Streams (default: 1)
#   - PRESENTATION_REPLICAS=3   # Number of presentation generators (default: 1)
#   - MEDIA_REPLICAS=2          # Number of media generators (default: 1)
#   - MAX_PARALLEL_LECTURES=3   # Lectures per worker concurrently (default: 3)
#
# Distributed Queue Architecture (USE_DISTRIBUTED_QUEUE=true):
#   Phase 1: course-worker consumes from RabbitMQ → creates lecture jobs
#   Phase 2: lecture-worker consumes from Redis Streams → generates each lecture
#   Phase 3: finalization-worker consumes from Redis Streams → assembles final course
#
# Recommended for 8 cores / 24GB RAM:
#   COURSE_REPLICAS=6, LECTURE_REPLICAS=4, FINALIZATION_REPLICAS=1,
#   PRESENTATION_REPLICAS=3, MEDIA_REPLICAS=2
# ===========================================

services:
  # ===========================================
  # COURSE WORKER
  # ===========================================
  # Consumes jobs from RabbitMQ and processes course generation
  course-worker:
    build:
      context: ./services/course-generator
      dockerfile: Dockerfile
    # Override command to run worker instead of API server
    command: python -m services.course_worker
    environment:
      # ===== Database (read-only, for RAG) =====
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${MAIN_SERVER_HOST}:5432/${DB_NAME}

      # ===== Message Queue (RabbitMQ on main server) =====
      - RABBITMQ_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@${MAIN_SERVER_HOST}:5672/

      # ===== Status Storage (Redis on main server) =====
      - REDIS_URL=redis://:${REDIS_PASSWORD}@${MAIN_SERVER_HOST}:6379/7

      # ===== LLM Provider Configuration =====
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY:-}
      - XAI_API_KEY=${XAI_API_KEY:-}

      # ===== Object Storage (MinIO on main server via HTTPS) =====
      # NOTE: STORAGE_ENDPOINT must NOT include /storage - boto3 adds bucket name after endpoint
      # nginx location /storage/ proxies to MinIO, so bucket name "storage" becomes the path prefix
      - STORAGE_ENABLED=${STORAGE_ENABLED:-true}
      - STORAGE_ENDPOINT=https://${MAIN_SERVER_HOST}
      - STORAGE_ACCESS_KEY=${MINIO_ROOT_USER:-viralify}
      - STORAGE_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - STORAGE_PUBLIC_URL=https://${MAIN_SERVER_HOST}/storage

      # ===== Internal Services (local on worker server) =====
      - PRESENTATION_GENERATOR_URL=http://presentation-generator:8006
      - MEDIA_GENERATOR_URL=http://media-generator:8004

      # ===== Misc =====
      - VECTOR_BACKEND=pgvector
      - PYTHONPATH=/app
    volumes:
      - course_output:/app/output
      - ./services/shared:/app/shared:ro
    deploy:
      replicas: ${COURSE_REPLICAS:-${WORKER_REPLICAS:-1}}
      resources:
        limits:
          memory: 2G
    depends_on:
      presentation-generator:
        condition: service_healthy
      media-generator:
        condition: service_healthy
    restart: always
    healthcheck:
      test: ["CMD", "pgrep", "-f", "course_worker"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - worker-network

  # ===========================================
  # LECTURE WORKER (Redis Streams consumer)
  # ===========================================
  # Phase 2: Consumes individual lecture jobs from Redis Streams
  # Generates lecture content (script, slides, video) via presentation-generator
  lecture-worker:
    build:
      context: ./services/course-generator
      dockerfile: Dockerfile
    # Override command to run lecture worker instead of API server
    command: python -m services.lecture_worker
    environment:
      # ===== Status Storage (Redis on main server - lecture_jobs stream) =====
      - REDIS_URL=redis://:${REDIS_PASSWORD}@${MAIN_SERVER_HOST}:6379/7

      # ===== LLM Provider Configuration =====
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY:-}
      - XAI_API_KEY=${XAI_API_KEY:-}

      # ===== Object Storage (MinIO on main server via HTTPS) =====
      - STORAGE_ENABLED=${STORAGE_ENABLED:-true}
      - STORAGE_ENDPOINT=https://${MAIN_SERVER_HOST}
      - STORAGE_ACCESS_KEY=${MINIO_ROOT_USER:-viralify}
      - STORAGE_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - STORAGE_PUBLIC_URL=https://${MAIN_SERVER_HOST}/storage

      # ===== Internal Services (local on worker server) =====
      - PRESENTATION_GENERATOR_URL=http://presentation-generator:8006
      - MEDIA_GENERATOR_URL=http://media-generator:8004

      # ===== Worker Configuration =====
      - MAX_PARALLEL_LECTURES=${MAX_PARALLEL_LECTURES:-3}
      - WORKER_ID=${WORKER_ID:-}
      - PYTHONPATH=/app
    volumes:
      - course_output:/app/output
      - ./services/shared:/app/shared:ro
    deploy:
      replicas: ${LECTURE_REPLICAS:-1}
      resources:
        limits:
          memory: 2G
    depends_on:
      presentation-generator:
        condition: service_healthy
      media-generator:
        condition: service_healthy
    restart: always
    healthcheck:
      test: ["CMD", "pgrep", "-f", "lecture_worker"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - worker-network

  # ===========================================
  # FINALIZATION WORKER (Redis Streams consumer)
  # ===========================================
  # Phase 3: Consumes finalization jobs, assembles final course package
  # Triggered when all lectures in a course are complete
  finalization-worker:
    build:
      context: ./services/course-generator
      dockerfile: Dockerfile
    # Override command to run finalization worker
    command: python -m services.course_finalizer
    environment:
      # ===== Status Storage (Redis on main server - finalization_jobs stream) =====
      - REDIS_URL=redis://:${REDIS_PASSWORD}@${MAIN_SERVER_HOST}:6379/7

      # ===== LLM Provider Configuration (for quiz generation) =====
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}

      # ===== Object Storage (MinIO on main server via HTTPS) =====
      - STORAGE_ENABLED=${STORAGE_ENABLED:-true}
      - STORAGE_ENDPOINT=https://${MAIN_SERVER_HOST}
      - STORAGE_ACCESS_KEY=${MINIO_ROOT_USER:-viralify}
      - STORAGE_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - STORAGE_PUBLIC_URL=https://${MAIN_SERVER_HOST}/storage

      # ===== Output Directory =====
      - COURSE_OUTPUT_DIR=/app/output/courses
      - WORKER_ID=${WORKER_ID:-}
      - PYTHONPATH=/app
    volumes:
      - course_output:/app/output
      - ./services/shared:/app/shared:ro
    deploy:
      replicas: ${FINALIZATION_REPLICAS:-1}
      resources:
        limits:
          memory: 2G
    restart: always
    healthcheck:
      test: ["CMD", "pgrep", "-f", "course_finalizer"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - worker-network

  # ===========================================
  # COURSE DLQ WORKER (Dead Letter Queue)
  # ===========================================
  # Retries failed jobs from the DLQ with exponential backoff
  course-dlq-worker:
    build:
      context: ./services/course-generator
      dockerfile: Dockerfile
    command: python -m services.course_dlq_worker
    environment:
      # ===== Database (read-only, for RAG) =====
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${MAIN_SERVER_HOST}:5432/${DB_NAME}

      # ===== Message Queue (RabbitMQ on main server) =====
      - RABBITMQ_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@${MAIN_SERVER_HOST}:5672/

      # ===== Status Storage (Redis on main server) =====
      - REDIS_URL=redis://:${REDIS_PASSWORD}@${MAIN_SERVER_HOST}:6379/7

      # ===== DLQ Configuration =====
      - DLQ_RETRY_DELAY_SECONDS=${DLQ_RETRY_DELAY_SECONDS:-30}

      # ===== LLM Provider Configuration =====
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY:-}
      - XAI_API_KEY=${XAI_API_KEY:-}

      # ===== Object Storage (MinIO on main server via HTTPS) =====
      # NOTE: STORAGE_ENDPOINT must NOT include /storage - boto3 adds bucket name after endpoint
      - STORAGE_ENABLED=${STORAGE_ENABLED:-true}
      - STORAGE_ENDPOINT=https://${MAIN_SERVER_HOST}
      - STORAGE_ACCESS_KEY=${MINIO_ROOT_USER:-viralify}
      - STORAGE_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - STORAGE_PUBLIC_URL=https://${MAIN_SERVER_HOST}/storage

      # ===== Internal Services (local on worker server) =====
      - PRESENTATION_GENERATOR_URL=http://presentation-generator:8006
      - MEDIA_GENERATOR_URL=http://media-generator:8004

      # ===== Misc =====
      - VECTOR_BACKEND=pgvector
      - PYTHONPATH=/app
    volumes:
      - course_output:/app/output
      - ./services/shared:/app/shared:ro
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 2G
    depends_on:
      presentation-generator:
        condition: service_healthy
      media-generator:
        condition: service_healthy
    restart: always
    healthcheck:
      test: ["CMD", "pgrep", "-f", "course_dlq_worker"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - worker-network

  # ===========================================
  # PRESENTATION GENERATOR (local to worker)
  # ===========================================
  presentation-generator:
    build:
      context: ./services/presentation-generator
      dockerfile: Dockerfile
    environment:
      # ===== Database (for job storage) =====
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${MAIN_SERVER_HOST}:5432/${DB_NAME}

      # ===== Redis (local job store) =====
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis-local:6379/6
      - REDIS_HOST=redis-local
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_worker_2024}
      - REDIS_JOB_DB=6

      # ===== Performance Tuning =====
      - UVICORN_WORKERS=1
      - FFMPEG_MAX_CONCURRENT=2
      - FFMPEG_PRESET=ultrafast

      # ===== LLM Provider =====
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - GROQ_API_KEY=${GROQ_API_KEY:-}

      # ===== Internal Services =====
      - MEDIA_GENERATOR_URL=http://media-generator:8004
      - VISUAL_GENERATOR_URL=http://visual-generator:8003
      - DIAGRAMS_GENERATOR_URL=http://diagrams-generator:8009
      - NEXUS_ENGINE_URL=http://nexus-engine:8011
      - USE_NEXUS_CODE_ENHANCEMENT=true
      - SERVICE_URL=http://presentation-generator:8006

      # ===== PPTX Service (PptxGenJS) =====
      - USE_PPTX_SERVICE=true
      - PPTX_SERVICE_URL=http://pptx-service:8013

      # ===== Public URLs (for video access) =====
      - PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-}
      - PUBLIC_MEDIA_URL=${PUBLIC_MEDIA_URL:-}

      # ===== Object Storage (MinIO via Nginx HTTPS proxy) =====
      # Workers upload via HTTPS through nginx (no direct port 9000 access needed)
      # NOTE: STORAGE_ENDPOINT must NOT include /storage - boto3 adds bucket name after endpoint
      - STORAGE_ENABLED=${STORAGE_ENABLED:-true}
      - STORAGE_ENDPOINT=${STORAGE_ENDPOINT:-https://olsitec.com}
      - STORAGE_ACCESS_KEY=${MINIO_ROOT_USER:-viralify}
      - STORAGE_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - STORAGE_PUBLIC_URL=${STORAGE_PUBLIC_URL:-https://olsitec.com/storage}

      # ===== Video Sync to Production (legacy, deprecated) =====
      - VIDEO_SYNC_ENABLED=${VIDEO_SYNC_ENABLED:-false}
      - VIDEO_SYNC_HOST=${VIDEO_SYNC_HOST:-}
      - VIDEO_SYNC_USER=${VIDEO_SYNC_USER:-ubuntu}
      - VIDEO_SYNC_PATH=${VIDEO_SYNC_PATH:-/var/lib/docker/volumes/repo_media_generator_videos/_data}
      - VIDEO_SYNC_SSH_KEY=${VIDEO_SYNC_SSH_KEY:-}
    volumes:
      - presentation_data:/tmp/presentations
      - media_videos:/tmp/viralify/videos
      - ./services/shared:/app/shared:ro
      # SSH keys for video sync to production
      - ~/.ssh:/root/.ssh:ro
    deploy:
      replicas: ${PRESENTATION_REPLICAS:-1}
      resources:
        limits:
          memory: 3G
    depends_on:
      pptx-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/health"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 30s
    restart: always
    networks:
      - worker-network

  # ===========================================
  # MEDIA GENERATOR (local to worker)
  # ===========================================
  media-generator:
    build:
      context: ./services/media-generator
      dockerfile: Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY:-}
      - CLOUDINARY_URL=${CLOUDINARY_URL:-}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN:-}
    volumes:
      - media_videos:/tmp/viralify/videos
      - media_audio:/tmp/viralify/audio
    deploy:
      replicas: ${MEDIA_REPLICAS:-1}
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: always
    networks:
      - worker-network

  # ===========================================
  # VISUAL GENERATOR (local to worker)
  # ===========================================
  visual-generator:
    build:
      context: ./services/visual-generator
      dockerfile: Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN:-}
    volumes:
      - visual_output:/tmp/visual-generator
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: always
    networks:
      - worker-network

  # ===========================================
  # NEXUS ENGINE (pedagogical code generation)
  # ===========================================
  nexus-engine:
    build:
      context: ./services/nexus-engine
      dockerfile: Dockerfile
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-180}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      - PORT=8011
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: always
    networks:
      - worker-network

  # ===========================================
  # DIAGRAMS GENERATOR (local to worker)
  # ===========================================
  diagrams-generator:
    build:
      context: ./services/diagrams-generator
      dockerfile: Dockerfile
    volumes:
      - diagrams_output:/tmp/diagrams
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8009/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: always
    networks:
      - worker-network

  # ===========================================
  # PPTX SERVICE (PptxGenJS + LibreOffice)
  # ===========================================
  # Generates professional PPTX files and converts to PNG
  # Uses PptxGenJS for slide generation, LibreOffice for PNG export
  pptx-service:
    build:
      context: ./services/pptx-service
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - PORT=8013
      - PPTX_OUTPUT_DIR=/tmp/viralify/pptx
      - PPTX_IMAGES_DIR=/tmp/viralify/pptx-images
    volumes:
      - pptx_output:/tmp/viralify/pptx
      - pptx_images:/tmp/viralify/pptx-images
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8013/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: always
    networks:
      - worker-network

  # ===========================================
  # LOCAL REDIS (for presentation job store)
  # ===========================================
  redis-local:
    image: redis:7-alpine
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis_worker_2024}
    volumes:
      - redis_local_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-redis_worker_2024}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - worker-network

# ===========================================
# VOLUMES
# ===========================================
volumes:
  course_output:
  presentation_data:
  media_videos:
  media_audio:
  visual_output:
  diagrams_output:
  pptx_output:
  pptx_images:
  redis_local_data:

# ===========================================
# NETWORKS
# ===========================================
networks:
  worker-network:
    driver: bridge
